{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess crawler data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cna_path = \"/mnt/disk3/m10615110/crawler_data/cna_news/\"\n",
    "cts_path = \"/mnt/disk3/m10615110/crawler_data/cts_news/\"\n",
    "udn_path = \"/mnt/disk3/m10615110/crawler_data/udn_news/\"\n",
    "ptt_path = \"/mnt/disk3/m10615110/crawler_data/ptt_gossiping/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cna data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ac966d884f4b1dadd3ef2e4f24a93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2506), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total 76537 news\n"
     ]
    }
   ],
   "source": [
    "file_list = list(Path(cna_path).glob('*.json'))\n",
    "cna_news = {}\n",
    "\n",
    "for i in tnrange(len(file_list)):\n",
    "    \n",
    "    if 'focus' in file_list[i].stem:\n",
    "        with open(file_list[i], 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for title, article in data.items():\n",
    "            right_gua = article.find('）')\n",
    "            article = article[right_gua+1:]\n",
    "            article = article[::-1]\n",
    "            last_period = article.find('。')\n",
    "            article = article[last_period:]\n",
    "            article = article[::-1]\n",
    "            if title not in cna_news:\n",
    "                cna_news[title] = article\n",
    "                \n",
    "    if 'news' in file_list[i].stem:\n",
    "        with open(file_list[i], 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        for cate, articles in data.items():\n",
    "            for title, article in articles.items():\n",
    "                right_gua = article.find('）')\n",
    "                article = article[right_gua+1:]\n",
    "                article = article[::-1]\n",
    "                last_period = article.find('。')\n",
    "                article = article[last_period:]\n",
    "                article = article[::-1]\n",
    "                if title not in cna_news:\n",
    "                    cna_news[title] = article\n",
    "    \n",
    "print('total {} news'.format(len(cna_news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c7df59a0fe4c91a9b56713a395fb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2447), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total 29114 news\n"
     ]
    }
   ],
   "source": [
    "file_list = list(Path(cts_path).glob('*.json'))\n",
    "cts_news = {}\n",
    "\n",
    "for i in tnrange(len(file_list)):\n",
    "    \n",
    "    if 'hots' in file_list[i].stem:\n",
    "        try:\n",
    "            with open(file_list[i], 'r') as f:\n",
    "                data = json.load(f)\n",
    "            for title, article in data.items():\n",
    "                if title not in cts_news:\n",
    "                    cts_news[title] = article\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "    if 'news' in file_list[i].stem:\n",
    "        try:\n",
    "            with open(file_list[i], 'r') as f:\n",
    "                data = json.load(f)\n",
    "            for cate, articles in data.items():\n",
    "                for title, article in articles.items():\n",
    "                    first_slide = article.find('/')\n",
    "                    article = article[first_slide+1:].strip()\n",
    "                    if title not in cts_news:\n",
    "                        cts_news[title] = article\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print('total {} news'.format(len(cts_news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## udn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dd00a6b0fb41f9ba3b767541eccedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=785), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total 117625 news\n"
     ]
    }
   ],
   "source": [
    "file_list = list(Path(udn_path).glob('*.json'))\n",
    "udn_news = {}\n",
    "\n",
    "for i in tnrange(len(file_list)):\n",
    "    \n",
    "    if 'news' in file_list[i].stem:\n",
    "        try:\n",
    "            with open(file_list[i], 'r') as f:\n",
    "                data = json.load(f)\n",
    "            for cate, articles in data.items():\n",
    "                for title, article in articles.items():\n",
    "                    first_slide = title.find('／')\n",
    "                    title = title[first_slide+1:] if first_slide != -1 else title\n",
    "                    right_gua = title.find('】')\n",
    "                    title = title[right_gua+1:] if right_gua != -1 else title\n",
    "                    \n",
    "                    if title not in udn_news:\n",
    "                        udn_news[title] = article\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print('total {} news'.format(len(udn_news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ptt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdab2c3324c4cfab82c184df2c869dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=526), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_list = list(Path(ptt_path).glob('*.json'))\n",
    "ptt_articles = {}\n",
    "\n",
    "for i in tnrange(len(file_list)):\n",
    "    \n",
    "    try:\n",
    "        with open(file_list[0], 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for article_id, article in data.items():\n",
    "            title = article['article_title']\n",
    "            content = article['content']\n",
    "            content = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', content, flags=re.MULTILINE)\n",
    "            messages = ''\n",
    "            for message in article['messages']:\n",
    "                message_content = message['push_content']\n",
    "                messages += message_content\n",
    "                messages += '。'\n",
    "                \n",
    "            if title not in ptt_articles:\n",
    "                ptt_articles[title] = [content, messages]\n",
    "                \n",
    "            \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simply combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446552\n"
     ]
    }
   ],
   "source": [
    "total_data = []\n",
    "for title, article in cna_news.items():\n",
    "    total_data.append(title)\n",
    "    total_data.append(article)\n",
    "    \n",
    "for title, article in cts_news.items():\n",
    "    total_data.append(title)\n",
    "    total_data.append(article)\n",
    "    \n",
    "for title, article in udn_news.items():\n",
    "    total_data.append(title)\n",
    "    total_data.append(article) \n",
    "    \n",
    "# for title, [content, messages] in ptt_articles.items():\n",
    "#     total_data.append(title)\n",
    "#     total_data.append(content)\n",
    "#     total_data.append(messages)\n",
    "\n",
    "print(len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0811 02:44:51.347485 139809267230528 file_utils.py:41] PyTorch version 1.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0811 02:44:54.059484 139809267230528 tokenization_utils.py:979] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/m10615110/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "exp_dir = \"/mnt/disk3/m10615110/gpt2_chinese/exp/\"\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    \"\"\" Access dictionary keys like attribute \n",
    "        https://stackoverflow.com/questions/4984647/accessing-dict-keys-like-an-attribute\n",
    "    \"\"\"\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "opts = AttrDict()\n",
    "\n",
    "# Configure models\n",
    "opts.vocab_size = bert_tokenizer.vocab_size\n",
    "opts.emb = 768\n",
    "\n",
    "\n",
    "# Configure optimization\n",
    "opts.learning_rate = 1.5e-4\n",
    "opts.bert_lr = 5e-6\n",
    "opts.weight_decay = 0.01 # L2 weight regularization\n",
    "opts.max_grad_norm = 1.0\n",
    "\n",
    "opts.batch_size = 4\n",
    "\n",
    "# Configure training\n",
    "opts.max_seq_len = 512\n",
    "opts.num_epochs = 300\n",
    "opts.warmup_steps = 4000\n",
    "opts.gradient_accumulation = 20\n",
    "\n",
    "opts.load_pretrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset():\n",
    "    def __init__(self, total_data):\n",
    "        \n",
    "        print('='*50)\n",
    "        print('Dataset preprocessing log:')\n",
    "        self.sents = total_data\n",
    "        print('- Number of sentences: {}'.format(len(self.sents)))\n",
    "        count = 0\n",
    "        for sent in self.sents:\n",
    "            count += len(sent)\n",
    "        print('- Number of words: {}'.format(count))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sent = self.sents[index]\n",
    "        \n",
    "        return sent\n",
    "    \n",
    "def collate_fn(data):\n",
    "    \n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq)-1 for seq in seqs]\n",
    "        input_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
    "        target_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
    "        for i, seq in enumerate(seqs):\n",
    "            input_seqs[i, :len(seq)-1] = torch.LongTensor(seq[:-1])\n",
    "            target_seqs[i, :len(seq)-1] = torch.LongTensor(seq[1:])\n",
    "            \n",
    "        return input_seqs, target_seqs, lens\n",
    "    \n",
    "    def bert_tokenize(tokenizer, article, max_length=1024):\n",
    "    \n",
    "        sents = re.split('。|，| ', article)\n",
    "        while '' in sents:\n",
    "            sents.remove('')\n",
    "\n",
    "        bert_sent = '[CLS]'\n",
    "        for sent in sents:\n",
    "            bert_sent += sent\n",
    "            bert_sent += '[SEP]'\n",
    "\n",
    "        tokens = tokenizer.tokenize(bert_sent)\n",
    "\n",
    "        truncat_tokens = []\n",
    "        if len(tokens) < max_length:\n",
    "            truncat_tokens.append(tokens)\n",
    "        else:\n",
    "            truncat_tokens = []\n",
    "            while len(tokens) > max_length:\n",
    "                truncat_tokens.append(tokens[:max_length])\n",
    "                tokens = tokens[max_length:]\n",
    "            truncat_tokens.append(tokens)\n",
    "\n",
    "        return truncat_tokens\n",
    "    \n",
    "    sents = data\n",
    "    \n",
    "    bert_tokens = []\n",
    "    for sent in sents:\n",
    "        tokens = bert_tokenize(bert_tokenizer, sent, max_length=opts.max_seq_len)\n",
    "        bert_tokens.append(tokens[0])\n",
    "        \n",
    "    bert_idxs = []\n",
    "    for bert_token in bert_tokens:\n",
    "        idxs = bert_tokenizer.convert_tokens_to_ids(bert_token)\n",
    "        bert_idxs.append(idxs)\n",
    "        \n",
    "    input_seqs, target_seqs, lens = _pad_sequences(bert_idxs)\n",
    "    \n",
    "    return sents, bert_tokens, bert_idxs, input_seqs, target_seqs, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 202000810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424224 22328\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, dev_data = train_test_split(total_data, test_size=0.05, random_state=random_seed, shuffle=True)\n",
    "\n",
    "print(len(train_data), len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Dataset preprocessing log:\n",
      "- Number of sentences: 424224\n",
      "- Number of words: 132709780\n",
      "==================================================\n",
      "Dataset preprocessing log:\n",
      "- Number of sentences: 22328\n",
      "- Number of words: 7098516\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(train_data)\n",
    "dev_dataset = TextDataset(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "train_iter = DataLoader(dataset=train_dataset,\n",
    "                        batch_size=opts.batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=16,\n",
    "#                         sampler=train_sampler,\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "dev_iter = DataLoader(dataset=dev_dataset,\n",
    "                        batch_size=2,\n",
    "                        shuffle=False,\n",
    "                        num_workers=16,\n",
    "#                         sampler=train_sampler,\n",
    "                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0811 02:44:56.785063 139809267230528 configuration_utils.py:286] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-config.json from cache at /home/m10615110/.cache/torch/transformers/98aa65385e18b0efd17acd8bf64dcdf21406bb0c99c801c2d3c9f6bfd1f48f29.250d6dc755ccb17d19c7c1a7677636683aa35f0f6cb5461b3c0587bc091551a0\n",
      "I0811 02:44:56.787188 139809267230528 configuration_utils.py:322] Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I0811 02:44:58.652463 139809267230528 modeling_utils.py:612] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin from cache at /home/m10615110/.cache/torch/transformers/4b337a4f3b7d3e1518f799e238af607498c02938a3390152aaec7d4dabca5a02.8769029be4f66a5ae1055eefdd1d11621b901d510654266b8681719fff492d6e\n",
      "I0811 02:45:11.524793 139809267230528 modeling_utils.py:703] Weights of GPT2LMHeadModel not initialized from pretrained model: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias', 'lm_head.weight']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parms :  324995072\n",
      "trainable parms :  324995072\n"
     ]
    }
   ],
   "source": [
    "if opts.load_pretrain:\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "    model.resize_token_embeddings(bert_tokenizer.vocab_size)\n",
    "#     model.half()\n",
    "else:\n",
    "    gpt2_config = GPT2Config()\n",
    "    model = GPT2LMHeadModel(config=gpt2_config)\n",
    "    model.resize_token_embeddings(bert_tokenizer.vocab_size)\n",
    "#     model.half()\n",
    "\n",
    "print('total parms : ', sum(p.numel() for p in model.parameters()))\n",
    "print('trainable parms : ', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(21128, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=21128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "## distribute data parallel\n",
    "\n",
    "# dist_backend = 'nccl'\n",
    "# dist_url = 'tcp://127.0.0.1:45655'\n",
    "# world_size = 1\n",
    "# rank = 0\n",
    "\n",
    "# torch.distributed.init_process_group(backend=dist_backend, \n",
    "#                                      init_method=dist_url, \n",
    "#                                      world_size=world_size, \n",
    "#                                      rank=rank)\n",
    "\n",
    "\n",
    "# bertlm = torch.nn.parallel.DistributedDataParallel(bertlm, find_unused_parameters=False)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "# dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/disk3/m10615110/gpt2_chinese/exp/gpt2_medium_noptt_len512_batch_4_2020-08-11 02:45:14\n"
     ]
    }
   ],
   "source": [
    "last_epoch = -1\n",
    "model_name = 'gpt2_medium_noptt_len{}_batch_{}'.format(opts.max_seq_len, opts.batch_size)\n",
    "now = str(datetime.now()).split('.')[0]\n",
    "experiment_name = '{}_{}'.format(model_name, now)\n",
    "experiment_dir = Path(exp_dir) / experiment_name\n",
    "experiment_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2file(log_file, msg):\n",
    "    with open(log_file, 'a') as fw:\n",
    "        fw.write(msg)\n",
    "        fw.write('\\n')\n",
    "\n",
    "experiment_trainlog = experiment_dir / 'train_log.txt'\n",
    "experiment_devlog = experiment_dir / 'dev_log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00015\n",
      "5e-06\n"
     ]
    }
   ],
   "source": [
    "print(opts.learning_rate)\n",
    "print(opts.bert_lr)\n",
    "\n",
    "optimizer = transformers.AdamW([\n",
    "    {'params': model.module.parameters(), 'lr':opts.learning_rate},\n",
    "], lr=opts.learning_rate)\n",
    "\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                                                         num_warmup_steps=opts.warmup_steps, \n",
    "                                                         num_training_steps=len(train_iter)*opts.num_epochs)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=bert_tokenizer.pad_token_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.001, exp_dir=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.best_epoch = 0\n",
    "        self.exp_dir=Path(exp_dir)\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch)\n",
    "        elif score < self.best_score:\n",
    "#         elif score < self.best_score or score < self.best_score * (1-self.delta):\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                msg = 'best epoch : {}'.format(self.best_epoch)\n",
    "                print(msg)\n",
    "                log2file(self.exp_dir / 'train_log.txt', msg)\n",
    "                (self.exp_dir / 'best_model').symlink_to(self.exp_dir / 'epoch_{}.mdl'.format(self.best_epoch))\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(val_loss, model, epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, epoch):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            msg = f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})'\n",
    "            print(msg)\n",
    "            log2file(self.exp_dir / 'train_log.txt', msg)\n",
    "#         torch.save(model.state_dict(), self.exp_dir / 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/disk3/m10615110/gpt2_chinese/exp/gpt2_medium_noptt_len512_batch_4_2020-08-11 02:45:14')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- vocab_size: 21128\n",
      "- emb: 768\n",
      "- learning_rate: 0.00015\n",
      "- bert_lr: 5e-06\n",
      "- weight_decay: 0.01\n",
      "- max_grad_norm: 1.0\n",
      "- batch_size: 4\n",
      "- max_seq_len: 512\n",
      "- num_epochs: 300\n",
      "- warmup_steps: 4000\n",
      "- gradient_accumulation: 20\n",
      "- load_pretrain: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ec3dce85374090b96126c79f86af28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=106056), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a031ae9ed9499094cacedca82eb8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11164), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "optim : \n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 0.00015\n",
      "    lr: 0.0\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "check point :  [10605, 21211, 31816, 42422, 53028, 63633, 74239, 84844, 95450]\n",
      "==================================================\n",
      "TRAIN | Batch 10605/106056 | Mean Loss 7.35276 | time cost 8167 s\n",
      "DEV   | Batch 10605/106056 | Mean Loss 6.12653 | Total time cost 3420 s\n",
      "Validation loss decreased (inf --> 6.126530)\n",
      "TRAIN | Batch 21211/106056 | Mean Loss 4.49267 | time cost 11608 s\n",
      "DEV   | Batch 21211/106056 | Mean Loss 5.66984 | Total time cost 3425 s\n",
      "Validation loss decreased (6.126530 --> 5.669838)\n",
      "TRAIN | Batch 31816/106056 | Mean Loss 2.75569 | time cost 11589 s\n",
      "DEV   | Batch 31816/106056 | Mean Loss 5.21175 | Total time cost 3423 s\n",
      "Validation loss decreased (5.669838 --> 5.211749)\n",
      "TRAIN | Batch 42422/106056 | Mean Loss 1.90314 | time cost 11616 s\n",
      "DEV   | Batch 42422/106056 | Mean Loss 4.86590 | Total time cost 3429 s\n",
      "Validation loss decreased (5.211749 --> 4.865902)\n",
      "TRAIN | Batch 53028/106056 | Mean Loss 1.43188 | time cost 11630 s\n",
      "DEV   | Batch 53028/106056 | Mean Loss 4.65325 | Total time cost 3426 s\n",
      "Validation loss decreased (4.865902 --> 4.653249)\n",
      "TRAIN | Batch 63633/106056 | Mean Loss 1.14138 | time cost 11597 s\n",
      "DEV   | Batch 63633/106056 | Mean Loss 4.47039 | Total time cost 3423 s\n",
      "Validation loss decreased (4.653249 --> 4.470387)\n",
      "TRAIN | Batch 74239/106056 | Mean Loss 0.94212 | time cost 11603 s\n",
      "DEV   | Batch 74239/106056 | Mean Loss 4.29911 | Total time cost 3423 s\n",
      "Validation loss decreased (4.470387 --> 4.299109)\n",
      "TRAIN | Batch 84844/106056 | Mean Loss 0.79496 | time cost 11619 s\n",
      "DEV   | Batch 84844/106056 | Mean Loss 4.15857 | Total time cost 3427 s\n",
      "Validation loss decreased (4.299109 --> 4.158573)\n",
      "TRAIN | Batch 95450/106056 | Mean Loss 0.68458 | time cost 11625 s\n",
      "DEV   | Batch 95450/106056 | Mean Loss 4.03767 | Total time cost 3427 s\n",
      "Validation loss decreased (4.158573 --> 4.037673)\n",
      "TRAIN | Epoch 0/300 | Mean Loss 0.59752 | Total time cost 112648 s\n",
      "DEV   | Epoch 0/300 | Mean Loss 3.94222  | Total time cost 3420 s\n",
      "Validation loss decreased (4.037673 --> 3.942223)\n",
      "==================================================\n",
      "==================================================\n",
      "TRAIN | Batch 10605/106056 | Mean Loss 3.75321 | time cost 8162 s\n",
      "DEV   | Batch 10605/106056 | Mean Loss 3.84602 | Total time cost 3424 s\n",
      "Validation loss decreased (3.942223 --> 3.846017)\n",
      "TRAIN | Batch 21211/106056 | Mean Loss 2.84647 | time cost 11584 s\n",
      "DEV   | Batch 21211/106056 | Mean Loss 3.77120 | Total time cost 3428 s\n",
      "Validation loss decreased (3.846017 --> 3.771198)\n",
      "TRAIN | Batch 31816/106056 | Mean Loss 1.86323 | time cost 11614 s\n",
      "DEV   | Batch 31816/106056 | Mean Loss 3.70013 | Total time cost 3421 s\n",
      "Validation loss decreased (3.771198 --> 3.700131)\n",
      "TRAIN | Batch 42422/106056 | Mean Loss 1.37131 | time cost 11578 s\n",
      "DEV   | Batch 42422/106056 | Mean Loss 3.63175 | Total time cost 3426 s\n",
      "Validation loss decreased (3.700131 --> 3.631755)\n",
      "TRAIN | Batch 53028/106056 | Mean Loss 1.08007 | time cost 11574 s\n",
      "DEV   | Batch 53028/106056 | Mean Loss 3.58483 | Total time cost 3427 s\n",
      "Validation loss decreased (3.631755 --> 3.584831)\n",
      "TRAIN | Batch 63633/106056 | Mean Loss 0.88537 | time cost 11603 s\n",
      "DEV   | Batch 63633/106056 | Mean Loss 3.52959 | Total time cost 3426 s\n",
      "Validation loss decreased (3.584831 --> 3.529591)\n",
      "TRAIN | Batch 74239/106056 | Mean Loss 0.74686 | time cost 11605 s\n",
      "DEV   | Batch 74239/106056 | Mean Loss 3.48308 | Total time cost 3428 s\n",
      "Validation loss decreased (3.529591 --> 3.483076)\n",
      "TRAIN | Batch 84844/106056 | Mean Loss 0.64739 | time cost 11593 s\n",
      "DEV   | Batch 84844/106056 | Mean Loss 3.44722 | Total time cost 3427 s\n",
      "Validation loss decreased (3.483076 --> 3.447216)\n",
      "TRAIN | Batch 95450/106056 | Mean Loss 0.56800 | time cost 11596 s\n",
      "DEV   | Batch 95450/106056 | Mean Loss 3.40288 | Total time cost 3426 s\n",
      "Validation loss decreased (3.447216 --> 3.402885)\n",
      "TRAIN | Epoch 1/300 | Mean Loss 0.50695 | Total time cost 112498 s\n",
      "DEV   | Epoch 1/300 | Mean Loss 3.36325  | Total time cost 3423 s\n",
      "Validation loss decreased (3.402885 --> 3.363251)\n",
      "==================================================\n",
      "==================================================\n",
      "TRAIN | Batch 10605/106056 | Mean Loss 3.21219 | time cost 8193 s\n",
      "DEV   | Batch 10605/106056 | Mean Loss 3.33007 | Total time cost 3424 s\n",
      "Validation loss decreased (3.363251 --> 3.330067)\n",
      "TRAIN | Batch 21211/106056 | Mean Loss 2.46953 | time cost 11600 s\n",
      "DEV   | Batch 21211/106056 | Mean Loss 3.30359 | Total time cost 3422 s\n",
      "Validation loss decreased (3.330067 --> 3.303592)\n",
      "TRAIN | Batch 31816/106056 | Mean Loss 1.63335 | time cost 11610 s\n",
      "DEV   | Batch 31816/106056 | Mean Loss 3.27947 | Total time cost 3426 s\n",
      "Validation loss decreased (3.303592 --> 3.279470)\n",
      "TRAIN | Batch 42422/106056 | Mean Loss 1.21799 | time cost 11602 s\n",
      "DEV   | Batch 42422/106056 | Mean Loss 3.24960 | Total time cost 3428 s\n",
      "Validation loss decreased (3.279470 --> 3.249602)\n",
      "TRAIN | Batch 53028/106056 | Mean Loss 0.96526 | time cost 11630 s\n",
      "DEV   | Batch 53028/106056 | Mean Loss 3.22103 | Total time cost 3423 s\n",
      "Validation loss decreased (3.249602 --> 3.221029)\n",
      "TRAIN | Batch 63633/106056 | Mean Loss 0.79595 | time cost 11598 s\n",
      "DEV   | Batch 63633/106056 | Mean Loss 3.19414 | Total time cost 3422 s\n",
      "Validation loss decreased (3.221029 --> 3.194144)\n",
      "TRAIN | Batch 74239/106056 | Mean Loss 0.67572 | time cost 11630 s\n",
      "DEV   | Batch 74239/106056 | Mean Loss 3.17373 | Total time cost 3428 s\n",
      "Validation loss decreased (3.194144 --> 3.173727)\n",
      "TRAIN | Batch 84844/106056 | Mean Loss 0.59049 | time cost 11625 s\n",
      "DEV   | Batch 84844/106056 | Mean Loss 3.15544 | Total time cost 3418 s\n",
      "Validation loss decreased (3.173727 --> 3.155439)\n",
      "TRAIN | Batch 95450/106056 | Mean Loss 0.52073 | time cost 11592 s\n",
      "DEV   | Batch 95450/106056 | Mean Loss 3.13533 | Total time cost 3424 s\n",
      "Validation loss decreased (3.155439 --> 3.135327)\n",
      "TRAIN | Epoch 2/300 | Mean Loss 0.46621 | Total time cost 112685 s\n",
      "DEV   | Epoch 2/300 | Mean Loss 3.12123  | Total time cost 3433 s\n",
      "Validation loss decreased (3.135327 --> 3.121227)\n",
      "==================================================\n",
      "==================================================\n",
      "TRAIN | Batch 10605/106056 | Mean Loss 2.97845 | time cost 8172 s\n",
      "DEV   | Batch 10605/106056 | Mean Loss 3.10143 | Total time cost 3425 s\n",
      "Validation loss decreased (3.121227 --> 3.101426)\n",
      "TRAIN | Batch 21211/106056 | Mean Loss 2.29025 | time cost 11647 s\n",
      "DEV   | Batch 21211/106056 | Mean Loss 3.08073 | Total time cost 3425 s\n",
      "Validation loss decreased (3.101426 --> 3.080733)\n",
      "TRAIN | Batch 31816/106056 | Mean Loss 1.51944 | time cost 11607 s\n",
      "DEV   | Batch 31816/106056 | Mean Loss 3.06939 | Total time cost 3422 s\n",
      "Validation loss decreased (3.080733 --> 3.069386)\n",
      "TRAIN | Batch 42422/106056 | Mean Loss 1.13762 | time cost 11605 s\n",
      "DEV   | Batch 42422/106056 | Mean Loss 3.04877 | Total time cost 3421 s\n",
      "Validation loss decreased (3.069386 --> 3.048767)\n",
      "TRAIN | Batch 53028/106056 | Mean Loss 0.90288 | time cost 11642 s\n",
      "DEV   | Batch 53028/106056 | Mean Loss 3.03673 | Total time cost 3428 s\n",
      "Validation loss decreased (3.048767 --> 3.036725)\n",
      "TRAIN | Batch 63633/106056 | Mean Loss 0.74945 | time cost 11606 s\n",
      "DEV   | Batch 63633/106056 | Mean Loss 3.02143 | Total time cost 3423 s\n",
      "Validation loss decreased (3.036725 --> 3.021435)\n",
      "TRAIN | Batch 74239/106056 | Mean Loss 0.63997 | time cost 11632 s\n",
      "DEV   | Batch 74239/106056 | Mean Loss 3.01121 | Total time cost 3427 s\n",
      "Validation loss decreased (3.021435 --> 3.011206)\n",
      "TRAIN | Batch 84844/106056 | Mean Loss 0.55840 | time cost 11595 s\n",
      "DEV   | Batch 84844/106056 | Mean Loss 2.99905 | Total time cost 3423 s\n",
      "Validation loss decreased (3.011206 --> 2.999052)\n",
      "TRAIN | Batch 95450/106056 | Mean Loss 0.49391 | time cost 11600 s\n",
      "DEV   | Batch 95450/106056 | Mean Loss 2.98591 | Total time cost 3429 s\n",
      "Validation loss decreased (2.999052 --> 2.985908)\n",
      "TRAIN | Epoch 3/300 | Mean Loss 0.44273 | Total time cost 112739 s\n",
      "DEV   | Epoch 3/300 | Mean Loss 2.97064  | Total time cost 3419 s\n",
      "Validation loss decreased (2.985908 --> 2.970645)\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Batch 10605/106056 | Mean Loss 2.82035 | time cost 8177 s\n",
      "DEV   | Batch 10605/106056 | Mean Loss 2.95687 | Total time cost 3427 s\n",
      "Validation loss decreased (2.970645 --> 2.956875)\n",
      "TRAIN | Batch 21211/106056 | Mean Loss 2.18348 | time cost 11586 s\n",
      "DEV   | Batch 21211/106056 | Mean Loss 2.94529 | Total time cost 3438 s\n",
      "Validation loss decreased (2.956875 --> 2.945290)\n",
      "TRAIN | Batch 31816/106056 | Mean Loss 1.45058 | time cost 11615 s\n",
      "DEV   | Batch 31816/106056 | Mean Loss 2.93581 | Total time cost 3427 s\n",
      "Validation loss decreased (2.945290 --> 2.935807)\n",
      "TRAIN | Batch 42422/106056 | Mean Loss 1.08507 | time cost 11577 s\n",
      "DEV   | Batch 42422/106056 | Mean Loss 2.92653 | Total time cost 3425 s\n",
      "Validation loss decreased (2.935807 --> 2.926532)\n",
      "TRAIN | Batch 53028/106056 | Mean Loss 0.86466 | time cost 11602 s\n",
      "DEV   | Batch 53028/106056 | Mean Loss 2.91725 | Total time cost 3423 s\n",
      "Validation loss decreased (2.926532 --> 2.917250)\n",
      "TRAIN | Batch 63633/106056 | Mean Loss 0.71969 | time cost 11620 s\n",
      "DEV   | Batch 63633/106056 | Mean Loss 2.90336 | Total time cost 3418 s\n",
      "Validation loss decreased (2.917250 --> 2.903361)\n",
      "TRAIN | Batch 74239/106056 | Mean Loss 0.61388 | time cost 11604 s\n",
      "DEV   | Batch 74239/106056 | Mean Loss 2.89336 | Total time cost 3425 s\n",
      "Validation loss decreased (2.903361 --> 2.893356)\n",
      "TRAIN | Batch 84844/106056 | Mean Loss 0.53491 | time cost 11653 s\n",
      "DEV   | Batch 84844/106056 | Mean Loss 2.88235 | Total time cost 3424 s\n",
      "Validation loss decreased (2.893356 --> 2.882354)\n",
      "TRAIN | Batch 95450/106056 | Mean Loss 0.47362 | time cost 11635 s\n",
      "DEV   | Batch 95450/106056 | Mean Loss 2.87574 | Total time cost 3423 s\n",
      "Validation loss decreased (2.882354 --> 2.875736)\n",
      "TRAIN | Epoch 4/300 | Mean Loss 0.42540 | Total time cost 112690 s\n",
      "DEV   | Epoch 4/300 | Mean Loss 2.86608  | Total time cost 3432 s\n",
      "Validation loss decreased (2.875736 --> 2.866080)\n",
      "==================================================\n",
      "==================================================\n",
      "TRAIN | Batch 10605/106056 | Mean Loss 2.71275 | time cost 8183 s\n",
      "DEV   | Batch 10605/106056 | Mean Loss 2.86022 | Total time cost 3424 s\n",
      "Validation loss decreased (2.866080 --> 2.860224)\n",
      "TRAIN | Batch 21211/106056 | Mean Loss 2.10384 | time cost 11609 s\n",
      "DEV   | Batch 21211/106056 | Mean Loss 2.85172 | Total time cost 3426 s\n",
      "Validation loss decreased (2.860224 --> 2.851716)\n",
      "TRAIN | Batch 31816/106056 | Mean Loss 1.40102 | time cost 11612 s\n",
      "DEV   | Batch 31816/106056 | Mean Loss 2.84611 | Total time cost 3425 s\n",
      "Validation loss decreased (2.851716 --> 2.846110)\n",
      "TRAIN | Batch 42422/106056 | Mean Loss 1.04961 | time cost 11591 s\n",
      "DEV   | Batch 42422/106056 | Mean Loss 2.84034 | Total time cost 3425 s\n",
      "Validation loss decreased (2.846110 --> 2.840340)\n",
      "TRAIN | Batch 53028/106056 | Mean Loss 0.83745 | time cost 11603 s\n",
      "DEV   | Batch 53028/106056 | Mean Loss 2.83124 | Total time cost 3432 s\n",
      "Validation loss decreased (2.840340 --> 2.831244)\n",
      "TRAIN | Batch 63633/106056 | Mean Loss 0.69541 | time cost 11612 s\n",
      "DEV   | Batch 63633/106056 | Mean Loss 2.82141 | Total time cost 3423 s\n",
      "Validation loss decreased (2.831244 --> 2.821408)\n",
      "TRAIN | Batch 74239/106056 | Mean Loss 0.59504 | time cost 11621 s\n",
      "DEV   | Batch 74239/106056 | Mean Loss 2.81300 | Total time cost 3425 s\n",
      "Validation loss decreased (2.821408 --> 2.812998)\n",
      "TRAIN | Batch 84844/106056 | Mean Loss 0.51845 | time cost 11609 s\n",
      "DEV   | Batch 84844/106056 | Mean Loss 2.80950 | Total time cost 3423 s\n",
      "Validation loss decreased (2.812998 --> 2.809495)\n",
      "TRAIN | Batch 95450/106056 | Mean Loss 0.46082 | time cost 11622 s\n",
      "DEV   | Batch 95450/106056 | Mean Loss 2.80125 | Total time cost 3433 s\n",
      "Validation loss decreased (2.809495 --> 2.801254)\n",
      "TRAIN | Epoch 5/300 | Mean Loss 0.41380 | Total time cost 112683 s\n",
      "DEV   | Epoch 5/300 | Mean Loss 2.79744  | Total time cost 3419 s\n",
      "Validation loss decreased (2.801254 --> 2.797435)\n",
      "==================================================\n",
      "==================================================\n",
      "TRAIN | Batch 10605/106056 | Mean Loss 2.62099 | time cost 8184 s\n",
      "DEV   | Batch 10605/106056 | Mean Loss 2.78997 | Total time cost 3420 s\n",
      "Validation loss decreased (2.797435 --> 2.789966)\n",
      "TRAIN | Batch 21211/106056 | Mean Loss 2.04828 | time cost 11591 s\n",
      "DEV   | Batch 21211/106056 | Mean Loss 2.78064 | Total time cost 3428 s\n",
      "Validation loss decreased (2.789966 --> 2.780644)\n",
      "TRAIN | Batch 31816/106056 | Mean Loss 1.35985 | time cost 11604 s\n",
      "DEV   | Batch 31816/106056 | Mean Loss 2.77737 | Total time cost 3425 s\n",
      "Validation loss decreased (2.780644 --> 2.777375)\n",
      "TRAIN | Batch 42422/106056 | Mean Loss 1.02134 | time cost 11615 s\n",
      "DEV   | Batch 42422/106056 | Mean Loss 2.77009 | Total time cost 3429 s\n",
      "Validation loss decreased (2.777375 --> 2.770093)\n",
      "TRAIN | Batch 53028/106056 | Mean Loss 0.81463 | time cost 11628 s\n",
      "DEV   | Batch 53028/106056 | Mean Loss 2.76526 | Total time cost 3420 s\n",
      "Validation loss decreased (2.770093 --> 2.765261)\n",
      "TRAIN | Batch 63633/106056 | Mean Loss 0.67923 | time cost 11600 s\n",
      "DEV   | Batch 63633/106056 | Mean Loss 2.76469 | Total time cost 3438 s\n",
      "Validation loss decreased (2.765261 --> 2.764694)\n",
      "TRAIN | Batch 74239/106056 | Mean Loss 0.58017 | time cost 11622 s\n",
      "DEV   | Batch 74239/106056 | Mean Loss 2.75144 | Total time cost 3421 s\n",
      "Validation loss decreased (2.764694 --> 2.751439)\n",
      "TRAIN | Batch 84844/106056 | Mean Loss 0.50704 | time cost 11603 s\n",
      "DEV   | Batch 84844/106056 | Mean Loss 2.74856 | Total time cost 3426 s\n",
      "Validation loss decreased (2.751439 --> 2.748557)\n",
      "TRAIN | Batch 95450/106056 | Mean Loss 0.45038 | time cost 11618 s\n",
      "DEV   | Batch 95450/106056 | Mean Loss 2.74106 | Total time cost 3430 s\n",
      "Validation loss decreased (2.748557 --> 2.741056)\n",
      "TRAIN | Epoch 6/300 | Mean Loss 0.40395 | Total time cost 112676 s\n",
      "DEV   | Epoch 6/300 | Mean Loss 2.73833  | Total time cost 3427 s\n",
      "Validation loss decreased (2.741056 --> 2.738331)\n",
      "==================================================\n",
      "==================================================\n",
      "TRAIN | Batch 10605/106056 | Mean Loss 2.56193 | time cost 8208 s\n",
      "DEV   | Batch 10605/106056 | Mean Loss 2.73585 | Total time cost 3428 s\n",
      "Validation loss decreased (2.738331 --> 2.735854)\n",
      "TRAIN | Batch 21211/106056 | Mean Loss 2.00319 | time cost 11600 s\n",
      "DEV   | Batch 21211/106056 | Mean Loss 2.73007 | Total time cost 3427 s\n",
      "Validation loss decreased (2.735854 --> 2.730067)\n",
      "TRAIN | Batch 31816/106056 | Mean Loss 1.33144 | time cost 11621 s\n",
      "DEV   | Batch 31816/106056 | Mean Loss 2.72587 | Total time cost 3430 s\n",
      "Validation loss decreased (2.730067 --> 2.725873)\n",
      "TRAIN | Batch 42422/106056 | Mean Loss 0.99971 | time cost 11598 s\n",
      "DEV   | Batch 42422/106056 | Mean Loss 2.72191 | Total time cost 3429 s\n",
      "Validation loss decreased (2.725873 --> 2.721912)\n",
      "TRAIN | Batch 53028/106056 | Mean Loss 0.79807 | time cost 11603 s\n",
      "DEV   | Batch 53028/106056 | Mean Loss 2.71571 | Total time cost 3475 s\n",
      "Validation loss decreased (2.721912 --> 2.715707)\n",
      "TRAIN | Batch 63633/106056 | Mean Loss 0.66313 | time cost 11711 s\n",
      "DEV   | Batch 63633/106056 | Mean Loss 2.70896 | Total time cost 3460 s\n",
      "Validation loss decreased (2.715707 --> 2.708965)\n",
      "TRAIN | Batch 74239/106056 | Mean Loss 0.56790 | time cost 11728 s\n",
      "DEV   | Batch 74239/106056 | Mean Loss 2.70464 | Total time cost 3464 s\n",
      "Validation loss decreased (2.708965 --> 2.704637)\n",
      "TRAIN | Batch 84844/106056 | Mean Loss 0.49654 | time cost 11732 s\n",
      "DEV   | Batch 84844/106056 | Mean Loss 2.70545 | Total time cost 3459 s\n",
      "EarlyStopping counter: 1 out of 50\n",
      "TRAIN | Batch 95450/106056 | Mean Loss 0.44078 | time cost 11722 s\n",
      "DEV   | Batch 95450/106056 | Mean Loss 2.69541 | Total time cost 3484 s\n",
      "Validation loss decreased (2.704637 --> 2.695411)\n",
      "TRAIN | Epoch 7/300 | Mean Loss 0.39574 | Total time cost 113291 s\n",
      "DEV   | Epoch 7/300 | Mean Loss 2.69377  | Total time cost 3463 s\n",
      "Validation loss decreased (2.695411 --> 2.693771)\n",
      "==================================================\n",
      "==================================================\n",
      "TRAIN | Batch 10605/106056 | Mean Loss 2.50864 | time cost 8078 s\n",
      "DEV   | Batch 10605/106056 | Mean Loss 2.69572 | Total time cost 3468 s\n",
      "EarlyStopping counter: 1 out of 50\n",
      "TRAIN | Batch 21211/106056 | Mean Loss 1.96131 | time cost 11550 s\n",
      "DEV   | Batch 21211/106056 | Mean Loss 2.68312 | Total time cost 3466 s\n",
      "Validation loss decreased (2.693771 --> 2.683124)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN | Batch 31816/106056 | Mean Loss 1.30705 | time cost 11543 s\n",
      "DEV   | Batch 31816/106056 | Mean Loss 2.68171 | Total time cost 3460 s\n",
      "Validation loss decreased (2.683124 --> 2.681710)\n",
      "TRAIN | Batch 42422/106056 | Mean Loss 0.98026 | time cost 11629 s\n",
      "DEV   | Batch 42422/106056 | Mean Loss 2.67883 | Total time cost 3463 s\n",
      "Validation loss decreased (2.681710 --> 2.678827)\n",
      "TRAIN | Batch 53028/106056 | Mean Loss 0.78209 | time cost 11728 s\n",
      "DEV   | Batch 53028/106056 | Mean Loss 2.67230 | Total time cost 3469 s\n",
      "Validation loss decreased (2.678827 --> 2.672295)\n",
      "TRAIN | Batch 63633/106056 | Mean Loss 0.65063 | time cost 11741 s\n",
      "DEV   | Batch 63633/106056 | Mean Loss 2.66769 | Total time cost 3476 s\n",
      "Validation loss decreased (2.672295 --> 2.667690)\n",
      "TRAIN | Batch 74239/106056 | Mean Loss 0.55713 | time cost 11742 s\n",
      "DEV   | Batch 74239/106056 | Mean Loss 2.66190 | Total time cost 3472 s\n",
      "Validation loss decreased (2.667690 --> 2.661897)\n",
      "TRAIN | Batch 84844/106056 | Mean Loss 0.48702 | time cost 11737 s\n",
      "DEV   | Batch 84844/106056 | Mean Loss 2.66158 | Total time cost 3460 s\n",
      "Validation loss decreased (2.661897 --> 2.661582)\n",
      "TRAIN | Batch 95450/106056 | Mean Loss 0.43292 | time cost 11729 s\n",
      "DEV   | Batch 95450/106056 | Mean Loss 2.65668 | Total time cost 3477 s\n",
      "Validation loss decreased (2.661582 --> 2.656684)\n",
      "TRAIN | Epoch 8/300 | Mean Loss 0.38859 | Total time cost 113228 s\n",
      "DEV   | Epoch 8/300 | Mean Loss 2.65018  | Total time cost 3462 s\n",
      "Validation loss decreased (2.656684 --> 2.650180)\n",
      "==================================================\n",
      "==================================================\n",
      "TRAIN | Batch 10605/106056 | Mean Loss 2.46197 | time cost 8258 s\n",
      "DEV   | Batch 10605/106056 | Mean Loss 2.65070 | Total time cost 3475 s\n",
      "EarlyStopping counter: 1 out of 50\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=50, verbose=True, exp_dir=str(experiment_dir))\n",
    "\n",
    "for k,v in opts.items():\n",
    "    log_msg = '- {}: {}'.format(k, v)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    print(log_msg)\n",
    "    \n",
    "pbar_train = tqdm_notebook(total=len(train_iter))\n",
    "pbar_dev = tqdm_notebook(total=len(dev_iter))\n",
    "    \n",
    "log_msg = '='*50\n",
    "print(log_msg)\n",
    "log2file(str(experiment_trainlog), log_msg)\n",
    "log_msg = 'optim : \\n' + str(optimizer)\n",
    "print(log_msg)   \n",
    "log2file(str(experiment_trainlog), log_msg)\n",
    "\n",
    "\n",
    "s = 10\n",
    "checkpoint = [int(len(train_iter)/s*i) for i in range(1, s)]\n",
    "\n",
    "oom_time = 0\n",
    "\n",
    "print('check point : ', checkpoint)\n",
    "\n",
    "for epoch in range(last_epoch+1,  opts.num_epochs, 1):\n",
    "    \n",
    "    pbar_train.reset()\n",
    "    pbar_dev.reset()\n",
    "    \n",
    "    log_msg = '='*50\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    loss_tracker = []\n",
    "    time_tracker = []\n",
    "    time_tracker.append(time.time())\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    \n",
    "    for iteration, batch in enumerate(train_iter):\n",
    "        \n",
    "        sents, bert_tokens, bert_idxs, input_seqs, target_seqs, lens = batch\n",
    "        \n",
    "        batch_size = input_seqs.size(0)\n",
    "        assert(batch_size == target_seqs.size(0))\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_seqs = input_seqs.cuda()\n",
    "    #         lens = lens.cuda()\n",
    "            target_seqs = target_seqs.cuda()\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        try:\n",
    "\n",
    "            outputs = model(input_seqs)\n",
    "\n",
    "            loss = criterion(outputs[0].view(outputs[0].size(0)*outputs[0].size(1), -1), \n",
    "                             target_seqs.view(target_seqs.size(0)*target_seqs.size(1)))\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), opts.max_grad_norm)\n",
    "\n",
    "            if (iteration + 1) % opts.gradient_accumulation == 0 or iteration == len(train_iter)-1:\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "            loss_tracker.append(loss.item()*batch_size)\n",
    "\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "        except RuntimeError as exception:\n",
    "            \n",
    "            if \"out of memory\" in str(exception):\n",
    "                oom_time += 1\n",
    "                log_msg = \"WARNING: ran out of memory,times: {}\".format(oom_time)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                torch.cuda.empty_cache()\n",
    "                if hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "            else:\n",
    "                log_msg = str(exception)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                raise exception\n",
    "                \n",
    "        #=================================\n",
    "        \n",
    "        if global_step in checkpoint:\n",
    "            \n",
    "            now_time = time.time()\n",
    "            time_tracker.append(time.time())\n",
    "            cur_avg_loss = np.sum(np.array(loss_tracker)) / (global_step * opts.batch_size)\n",
    "            log_msg = \"{} | Batch {:d}/{:d} | Mean Loss {:5.5f} | time cost {:d} s\"  \\\n",
    "                    .format('train'.upper(), global_step, len(train_iter), cur_avg_loss, int(time_tracker[-1] - time_tracker[-2]))\n",
    "            print(log_msg)\n",
    "            log2file(str(experiment_trainlog), log_msg)\n",
    "            now_percent = checkpoint.index(global_step)+1\n",
    "            torch.save(model.state_dict(), experiment_dir / 'epoch_{}_{}.mdl'.format(epoch-1, now_percent))\n",
    "            \n",
    "            loss_tracker = []\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            pbar_dev.reset()\n",
    "\n",
    "            for iteration, batch in enumerate(dev_iter):\n",
    "\n",
    "                sents, bert_tokens, bert_idxs, input_seqs, target_seqs, lens = batch\n",
    "\n",
    "                batch_size = input_seqs.size(0)\n",
    "                assert(batch_size == target_seqs.size(0))\n",
    "\n",
    "                if USE_CUDA:\n",
    "                    input_seqs = input_seqs.cuda()\n",
    "            #         lens = lens.cuda()\n",
    "                    target_seqs = target_seqs.cuda()\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                try:\n",
    "\n",
    "                    outputs = model(input_seqs)\n",
    "\n",
    "                    loss = criterion(outputs[0].view(outputs[0].size(0)*outputs[0].size(1), -1), \n",
    "                                     target_seqs.view(target_seqs.size(0)*target_seqs.size(1)))\n",
    "\n",
    "                    loss_tracker.append(loss.item()*batch_size)\n",
    "\n",
    "                except RuntimeError as exception:\n",
    "\n",
    "                    if \"out of memory\" in str(exception):\n",
    "                        oom_time += 1\n",
    "                        log_msg = \"WARNING: ran out of memory,times: {}\".format(oom_time)\n",
    "                        print(log_msg)   \n",
    "                        log2file(str(experiment_trainlog), log_msg)\n",
    "                        torch.cuda.empty_cache()\n",
    "                        if hasattr(torch.cuda, 'empty_cache'):\n",
    "                            torch.cuda.empty_cache()\n",
    "                    else:\n",
    "                        log_msg = str(exception)\n",
    "                        print(log_msg)   \n",
    "                        log2file(str(experiment_trainlog), log_msg)\n",
    "                        raise exception\n",
    "#                 torch.cuda.empty_cache()\n",
    "                        \n",
    "                pbar_dev.update(1)\n",
    "\n",
    "\n",
    "            total_time = time.time() - start\n",
    "\n",
    "            mean_loss = np.sum(np.array(loss_tracker)) / dev_dataset.__len__()\n",
    "            log_msg = \"{}   | Batch {:d}/{:d} | Mean Loss {:5.5f} | Total time cost {:d} s\"  \\\n",
    "                .format('dev'.upper(), global_step, len(train_iter), mean_loss, int(total_time))\n",
    "            print(log_msg)\n",
    "            log2file(str(experiment_trainlog), log_msg)\n",
    "\n",
    "            val_loss = mean_loss\n",
    "\n",
    "            early_stopping(val_loss, model, epoch)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "        global_step += 1\n",
    "        pbar_train.update(1)\n",
    "\n",
    "    \n",
    "    total_time = time.time() - time_tracker[0]    \n",
    "    mean_loss = np.sum(np.array(loss_tracker)) / train_dataset.__len__()\n",
    "    log_msg = \"{} | Epoch {:d}/{:d} | Mean Loss {:5.5f} | Total time cost {:d} s\"  \\\n",
    "        .format('train'.upper(), epoch, opts.num_epochs, mean_loss, int(total_time))\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "\n",
    "    #-----------------------\n",
    "\n",
    "    loss_tracker = []\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    pbar_dev.reset()\n",
    "\n",
    "    for iteration, batch in enumerate(dev_iter):\n",
    "\n",
    "        sents, bert_tokens, bert_idxs, input_seqs, target_seqs, lens = batch\n",
    "        \n",
    "        batch_size = input_seqs.size(0)\n",
    "        assert(batch_size == target_seqs.size(0))\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_seqs = input_seqs.cuda()\n",
    "    #         lens = lens.cuda()\n",
    "            target_seqs = target_seqs.cuda()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            outputs = model(input_seqs)\n",
    "\n",
    "            loss = criterion(outputs[0].view(outputs[0].size(0)*outputs[0].size(1), -1), \n",
    "                             target_seqs.view(target_seqs.size(0)*target_seqs.size(1)))\n",
    "\n",
    "            loss_tracker.append(loss.item()*batch_size)\n",
    "            \n",
    "        except RuntimeError as exception:\n",
    "            \n",
    "            if \"out of memory\" in str(exception):\n",
    "                oom_time += 1\n",
    "                log_msg = \"WARNING: ran out of memory,times: {}\".format(oom_time)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                torch.cuda.empty_cache()\n",
    "                if hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "            else:\n",
    "                log_msg = str(exception)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                raise exception\n",
    "                \n",
    "#         torch.cuda.empty_cache()\n",
    "        pbar_dev.update(1)\n",
    "\n",
    "    total_time = time.time() - start\n",
    "\n",
    "    mean_loss = np.sum(np.array(loss_tracker)) / dev_dataset.__len__()\n",
    "    log_msg = \"{}   | Epoch {:d}/{:d} | Mean Loss {:5.5f}  | Total time cost {:d} s\"  \\\n",
    "        .format('dev'.upper(), epoch, opts.num_epochs, mean_loss, int(total_time))\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    \n",
    "    val_loss = mean_loss\n",
    "    \n",
    "    early_stopping(val_loss, model, epoch)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "    torch.save(model.state_dict(), experiment_dir / 'epoch_{}.mdl'.format(epoch))\n",
    "\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess crawler data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cna_path = \"crawler_data/cna_news/\"\n",
    "cts_path = \"crawler_data/cts_news/\"\n",
    "udn_path = \"crawler_data/udn_news/\"\n",
    "ptt_path = \"crawler_data/ptt_gossiping/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cna data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = list(Path(cna_path).glob('*.json'))\n",
    "cna_news = {}\n",
    "\n",
    "for i in tnrange(len(file_list)):\n",
    "    \n",
    "    if 'focus' in file_list[i].stem:\n",
    "        with open(file_list[i], 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for title, article in data.items():\n",
    "            right_gua = article.find('）')\n",
    "            article = article[right_gua+1:]\n",
    "            article = article[::-1]\n",
    "            last_period = article.find('。')\n",
    "            article = article[last_period:]\n",
    "            article = article[::-1]\n",
    "            if title not in cna_news:\n",
    "                cna_news[title] = article\n",
    "                \n",
    "    if 'news' in file_list[i].stem:\n",
    "        with open(file_list[i], 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        for cate, articles in data.items():\n",
    "            for title, article in articles.items():\n",
    "                right_gua = article.find('）')\n",
    "                article = article[right_gua+1:]\n",
    "                article = article[::-1]\n",
    "                last_period = article.find('。')\n",
    "                article = article[last_period:]\n",
    "                article = article[::-1]\n",
    "                if title not in cna_news:\n",
    "                    cna_news[title] = article\n",
    "    \n",
    "print('total {} news'.format(len(cna_news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = list(Path(cts_path).glob('*.json'))\n",
    "cts_news = {}\n",
    "\n",
    "for i in tnrange(len(file_list)):\n",
    "    \n",
    "    if 'hots' in file_list[i].stem:\n",
    "        try:\n",
    "            with open(file_list[i], 'r') as f:\n",
    "                data = json.load(f)\n",
    "            for title, article in data.items():\n",
    "                if title not in cts_news:\n",
    "                    cts_news[title] = article\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "    if 'news' in file_list[i].stem:\n",
    "        try:\n",
    "            with open(file_list[i], 'r') as f:\n",
    "                data = json.load(f)\n",
    "            for cate, articles in data.items():\n",
    "                for title, article in articles.items():\n",
    "                    first_slide = article.find('/')\n",
    "                    article = article[first_slide+1:].strip()\n",
    "                    if title not in cts_news:\n",
    "                        cts_news[title] = article\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print('total {} news'.format(len(cts_news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## udn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = list(Path(udn_path).glob('*.json'))\n",
    "udn_news = {}\n",
    "\n",
    "for i in tnrange(len(file_list)):\n",
    "    \n",
    "    if 'news' in file_list[i].stem:\n",
    "        try:\n",
    "            with open(file_list[i], 'r') as f:\n",
    "                data = json.load(f)\n",
    "            for cate, articles in data.items():\n",
    "                for title, article in articles.items():\n",
    "                    first_slide = title.find('／')\n",
    "                    title = title[first_slide+1:] if first_slide != -1 else title\n",
    "                    right_gua = title.find('】')\n",
    "                    title = title[right_gua+1:] if right_gua != -1 else title\n",
    "                    \n",
    "                    if title not in udn_news:\n",
    "                        udn_news[title] = article\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print('total {} news'.format(len(udn_news)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ptt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = list(Path(ptt_path).glob('*.json'))\n",
    "ptt_articles = {}\n",
    "\n",
    "for i in tnrange(len(file_list)):\n",
    "    \n",
    "    try:\n",
    "        with open(file_list[0], 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for article_id, article in data.items():\n",
    "            title = article['article_title']\n",
    "            content = article['content']\n",
    "            content = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', content, flags=re.MULTILINE)\n",
    "            messages = ''\n",
    "            for message in article['messages']:\n",
    "                message_content = message['push_content']\n",
    "                messages += message_content\n",
    "                messages += '。'\n",
    "                \n",
    "            if title not in ptt_articles:\n",
    "                ptt_articles[title] = [content, messages]\n",
    "                \n",
    "            \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simply combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = []\n",
    "for title, article in cna_news.items():\n",
    "    total_data.append(title)\n",
    "    total_data.append(article)\n",
    "    \n",
    "for title, article in cts_news.items():\n",
    "    total_data.append(title)\n",
    "    total_data.append(article)\n",
    "    \n",
    "for title, article in udn_news.items():\n",
    "    total_data.append(title)\n",
    "    total_data.append(article) \n",
    "    \n",
    "# for title, [content, messages] in ptt_articles.items():\n",
    "#     total_data.append(title)\n",
    "#     total_data.append(content)\n",
    "#     total_data.append(messages)\n",
    "\n",
    "print(len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = \"gpt2_chinese/exp/\"\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    \"\"\" Access dictionary keys like attribute \n",
    "        https://stackoverflow.com/questions/4984647/accessing-dict-keys-like-an-attribute\n",
    "    \"\"\"\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "opts = AttrDict()\n",
    "\n",
    "# Configure models\n",
    "opts.vocab_size = bert_tokenizer.vocab_size\n",
    "opts.emb = 768\n",
    "\n",
    "\n",
    "# Configure optimization\n",
    "opts.learning_rate = 1.5e-4\n",
    "opts.bert_lr = 5e-6\n",
    "opts.weight_decay = 0.01 # L2 weight regularization\n",
    "opts.max_grad_norm = 1.0\n",
    "\n",
    "opts.batch_size = 4\n",
    "\n",
    "# Configure training\n",
    "opts.max_seq_len = 512\n",
    "opts.num_epochs = 300\n",
    "opts.warmup_steps = 4000\n",
    "opts.gradient_accumulation = 20\n",
    "\n",
    "opts.load_pretrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset():\n",
    "    def __init__(self, total_data):\n",
    "        \n",
    "        print('='*50)\n",
    "        print('Dataset preprocessing log:')\n",
    "        self.sents = total_data\n",
    "        print('- Number of sentences: {}'.format(len(self.sents)))\n",
    "        count = 0\n",
    "        for sent in self.sents:\n",
    "            count += len(sent)\n",
    "        print('- Number of words: {}'.format(count))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sent = self.sents[index]\n",
    "        \n",
    "        return sent\n",
    "    \n",
    "def collate_fn(data):\n",
    "    \n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq)-1 for seq in seqs]\n",
    "        input_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
    "        target_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
    "        for i, seq in enumerate(seqs):\n",
    "            input_seqs[i, :len(seq)-1] = torch.LongTensor(seq[:-1])\n",
    "            target_seqs[i, :len(seq)-1] = torch.LongTensor(seq[1:])\n",
    "            \n",
    "        return input_seqs, target_seqs, lens\n",
    "    \n",
    "    def bert_tokenize(tokenizer, article, max_length=1024):\n",
    "    \n",
    "        sents = re.split('。|，| ', article)\n",
    "        while '' in sents:\n",
    "            sents.remove('')\n",
    "\n",
    "        bert_sent = '[CLS]'\n",
    "        for sent in sents:\n",
    "            bert_sent += sent\n",
    "            bert_sent += '[SEP]'\n",
    "\n",
    "        tokens = tokenizer.tokenize(bert_sent)\n",
    "\n",
    "        truncat_tokens = []\n",
    "        if len(tokens) < max_length:\n",
    "            truncat_tokens.append(tokens)\n",
    "        else:\n",
    "            truncat_tokens = []\n",
    "            while len(tokens) > max_length:\n",
    "                truncat_tokens.append(tokens[:max_length])\n",
    "                tokens = tokens[max_length:]\n",
    "            truncat_tokens.append(tokens)\n",
    "\n",
    "        return truncat_tokens\n",
    "    \n",
    "    sents = data\n",
    "    \n",
    "    bert_tokens = []\n",
    "    for sent in sents:\n",
    "        tokens = bert_tokenize(bert_tokenizer, sent, max_length=opts.max_seq_len)\n",
    "        bert_tokens.append(tokens[0])\n",
    "        \n",
    "    bert_idxs = []\n",
    "    for bert_token in bert_tokens:\n",
    "        idxs = bert_tokenizer.convert_tokens_to_ids(bert_token)\n",
    "        bert_idxs.append(idxs)\n",
    "        \n",
    "    input_seqs, target_seqs, lens = _pad_sequences(bert_idxs)\n",
    "    \n",
    "    return sents, bert_tokens, bert_idxs, input_seqs, target_seqs, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 202000810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, dev_data = train_test_split(total_data, test_size=0.05, random_state=random_seed, shuffle=True)\n",
    "\n",
    "print(len(train_data), len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_data)\n",
    "dev_dataset = TextDataset(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "train_iter = DataLoader(dataset=train_dataset,\n",
    "                        batch_size=opts.batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=16,\n",
    "#                         sampler=train_sampler,\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "dev_iter = DataLoader(dataset=dev_dataset,\n",
    "                        batch_size=2,\n",
    "                        shuffle=False,\n",
    "                        num_workers=16,\n",
    "#                         sampler=train_sampler,\n",
    "                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.load_pretrain:\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "    model.resize_token_embeddings(bert_tokenizer.vocab_size)\n",
    "#     model.half()\n",
    "else:\n",
    "    gpt2_config = GPT2Config()\n",
    "    model = GPT2LMHeadModel(config=gpt2_config)\n",
    "    model.resize_token_embeddings(bert_tokenizer.vocab_size)\n",
    "#     model.half()\n",
    "\n",
    "print('total parms : ', sum(p.numel() for p in model.parameters()))\n",
    "print('trainable parms : ', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribute data parallel\n",
    "\n",
    "# dist_backend = 'nccl'\n",
    "# dist_url = 'tcp://127.0.0.1:45655'\n",
    "# world_size = 1\n",
    "# rank = 0\n",
    "\n",
    "# torch.distributed.init_process_group(backend=dist_backend, \n",
    "#                                      init_method=dist_url, \n",
    "#                                      world_size=world_size, \n",
    "#                                      rank=rank)\n",
    "\n",
    "\n",
    "# bertlm = torch.nn.parallel.DistributedDataParallel(bertlm, find_unused_parameters=False)\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "# dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = -1\n",
    "model_name = 'gpt2_medium_noptt_len{}_batch_{}'.format(opts.max_seq_len, opts.batch_size)\n",
    "now = str(datetime.now()).split('.')[0]\n",
    "experiment_name = '{}_{}'.format(model_name, now)\n",
    "experiment_dir = Path(exp_dir) / experiment_name\n",
    "experiment_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2file(log_file, msg):\n",
    "    with open(log_file, 'a') as fw:\n",
    "        fw.write(msg)\n",
    "        fw.write('\\n')\n",
    "\n",
    "experiment_trainlog = experiment_dir / 'train_log.txt'\n",
    "experiment_devlog = experiment_dir / 'dev_log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opts.learning_rate)\n",
    "print(opts.bert_lr)\n",
    "\n",
    "optimizer = transformers.AdamW([\n",
    "    {'params': model.module.parameters(), 'lr':opts.learning_rate},\n",
    "], lr=opts.learning_rate)\n",
    "\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                                                         num_warmup_steps=opts.warmup_steps, \n",
    "                                                         num_training_steps=len(train_iter)*opts.num_epochs)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=bert_tokenizer.pad_token_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.001, exp_dir=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.best_epoch = 0\n",
    "        self.exp_dir=Path(exp_dir)\n",
    "\n",
    "    def __call__(self, val_loss, model, epoch):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, epoch)\n",
    "        elif score < self.best_score:\n",
    "#         elif score < self.best_score or score < self.best_score * (1-self.delta):\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                msg = 'best epoch : {}'.format(self.best_epoch)\n",
    "                print(msg)\n",
    "                log2file(self.exp_dir / 'train_log.txt', msg)\n",
    "                (self.exp_dir / 'best_model').symlink_to(self.exp_dir / 'epoch_{}.mdl'.format(self.best_epoch))\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.save_checkpoint(val_loss, model, epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, epoch):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            msg = f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})'\n",
    "            print(msg)\n",
    "            log2file(self.exp_dir / 'train_log.txt', msg)\n",
    "#         torch.save(model.state_dict(), self.exp_dir / 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=50, verbose=True, exp_dir=str(experiment_dir))\n",
    "\n",
    "for k,v in opts.items():\n",
    "    log_msg = '- {}: {}'.format(k, v)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    print(log_msg)\n",
    "    \n",
    "pbar_train = tqdm_notebook(total=len(train_iter))\n",
    "pbar_dev = tqdm_notebook(total=len(dev_iter))\n",
    "    \n",
    "log_msg = '='*50\n",
    "print(log_msg)\n",
    "log2file(str(experiment_trainlog), log_msg)\n",
    "log_msg = 'optim : \\n' + str(optimizer)\n",
    "print(log_msg)   \n",
    "log2file(str(experiment_trainlog), log_msg)\n",
    "\n",
    "\n",
    "s = 10\n",
    "checkpoint = [int(len(train_iter)/s*i) for i in range(1, s)]\n",
    "\n",
    "oom_time = 0\n",
    "\n",
    "print('check point : ', checkpoint)\n",
    "\n",
    "for epoch in range(last_epoch+1,  opts.num_epochs, 1):\n",
    "    \n",
    "    pbar_train.reset()\n",
    "    pbar_dev.reset()\n",
    "    \n",
    "    log_msg = '='*50\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    loss_tracker = []\n",
    "    time_tracker = []\n",
    "    time_tracker.append(time.time())\n",
    "    \n",
    "    global_step = 0\n",
    "    \n",
    "    \n",
    "    for iteration, batch in enumerate(train_iter):\n",
    "        \n",
    "        sents, bert_tokens, bert_idxs, input_seqs, target_seqs, lens = batch\n",
    "        \n",
    "        batch_size = input_seqs.size(0)\n",
    "        assert(batch_size == target_seqs.size(0))\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_seqs = input_seqs.cuda()\n",
    "    #         lens = lens.cuda()\n",
    "            target_seqs = target_seqs.cuda()\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        try:\n",
    "\n",
    "            outputs = model(input_seqs)\n",
    "\n",
    "            loss = criterion(outputs[0].view(outputs[0].size(0)*outputs[0].size(1), -1), \n",
    "                             target_seqs.view(target_seqs.size(0)*target_seqs.size(1)))\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), opts.max_grad_norm)\n",
    "\n",
    "            if (iteration + 1) % opts.gradient_accumulation == 0 or iteration == len(train_iter)-1:\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "            loss_tracker.append(loss.item()*batch_size)\n",
    "\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "        except RuntimeError as exception:\n",
    "            \n",
    "            if \"out of memory\" in str(exception):\n",
    "                oom_time += 1\n",
    "                log_msg = \"WARNING: ran out of memory,times: {}\".format(oom_time)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                torch.cuda.empty_cache()\n",
    "                if hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "            else:\n",
    "                log_msg = str(exception)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                raise exception\n",
    "                \n",
    "        #=================================\n",
    "        \n",
    "        if global_step in checkpoint:\n",
    "            \n",
    "            now_time = time.time()\n",
    "            time_tracker.append(time.time())\n",
    "            cur_avg_loss = np.sum(np.array(loss_tracker)) / (global_step * opts.batch_size)\n",
    "            log_msg = \"{} | Batch {:d}/{:d} | Mean Loss {:5.5f} | time cost {:d} s\"  \\\n",
    "                    .format('train'.upper(), global_step, len(train_iter), cur_avg_loss, int(time_tracker[-1] - time_tracker[-2]))\n",
    "            print(log_msg)\n",
    "            log2file(str(experiment_trainlog), log_msg)\n",
    "            now_percent = checkpoint.index(global_step)+1\n",
    "            torch.save(model.state_dict(), experiment_dir / 'epoch_{}_{}.mdl'.format(epoch-1, now_percent))\n",
    "            \n",
    "            loss_tracker = []\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            pbar_dev.reset()\n",
    "\n",
    "            for iteration, batch in enumerate(dev_iter):\n",
    "\n",
    "                sents, bert_tokens, bert_idxs, input_seqs, target_seqs, lens = batch\n",
    "\n",
    "                batch_size = input_seqs.size(0)\n",
    "                assert(batch_size == target_seqs.size(0))\n",
    "\n",
    "                if USE_CUDA:\n",
    "                    input_seqs = input_seqs.cuda()\n",
    "            #         lens = lens.cuda()\n",
    "                    target_seqs = target_seqs.cuda()\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                try:\n",
    "\n",
    "                    outputs = model(input_seqs)\n",
    "\n",
    "                    loss = criterion(outputs[0].view(outputs[0].size(0)*outputs[0].size(1), -1), \n",
    "                                     target_seqs.view(target_seqs.size(0)*target_seqs.size(1)))\n",
    "\n",
    "                    loss_tracker.append(loss.item()*batch_size)\n",
    "\n",
    "                except RuntimeError as exception:\n",
    "\n",
    "                    if \"out of memory\" in str(exception):\n",
    "                        oom_time += 1\n",
    "                        log_msg = \"WARNING: ran out of memory,times: {}\".format(oom_time)\n",
    "                        print(log_msg)   \n",
    "                        log2file(str(experiment_trainlog), log_msg)\n",
    "                        torch.cuda.empty_cache()\n",
    "                        if hasattr(torch.cuda, 'empty_cache'):\n",
    "                            torch.cuda.empty_cache()\n",
    "                    else:\n",
    "                        log_msg = str(exception)\n",
    "                        print(log_msg)   \n",
    "                        log2file(str(experiment_trainlog), log_msg)\n",
    "                        raise exception\n",
    "#                 torch.cuda.empty_cache()\n",
    "                        \n",
    "                pbar_dev.update(1)\n",
    "\n",
    "\n",
    "            total_time = time.time() - start\n",
    "\n",
    "            mean_loss = np.sum(np.array(loss_tracker)) / dev_dataset.__len__()\n",
    "            log_msg = \"{}   | Batch {:d}/{:d} | Mean Loss {:5.5f} | Total time cost {:d} s\"  \\\n",
    "                .format('dev'.upper(), global_step, len(train_iter), mean_loss, int(total_time))\n",
    "            print(log_msg)\n",
    "            log2file(str(experiment_trainlog), log_msg)\n",
    "\n",
    "            val_loss = mean_loss\n",
    "\n",
    "            early_stopping(val_loss, model, epoch)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "        global_step += 1\n",
    "        pbar_train.update(1)\n",
    "\n",
    "    \n",
    "    total_time = time.time() - time_tracker[0]    \n",
    "    mean_loss = np.sum(np.array(loss_tracker)) / train_dataset.__len__()\n",
    "    log_msg = \"{} | Epoch {:d}/{:d} | Mean Loss {:5.5f} | Total time cost {:d} s\"  \\\n",
    "        .format('train'.upper(), epoch, opts.num_epochs, mean_loss, int(total_time))\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "\n",
    "    #-----------------------\n",
    "\n",
    "    loss_tracker = []\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    pbar_dev.reset()\n",
    "\n",
    "    for iteration, batch in enumerate(dev_iter):\n",
    "\n",
    "        sents, bert_tokens, bert_idxs, input_seqs, target_seqs, lens = batch\n",
    "        \n",
    "        batch_size = input_seqs.size(0)\n",
    "        assert(batch_size == target_seqs.size(0))\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_seqs = input_seqs.cuda()\n",
    "    #         lens = lens.cuda()\n",
    "            target_seqs = target_seqs.cuda()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            outputs = model(input_seqs)\n",
    "\n",
    "            loss = criterion(outputs[0].view(outputs[0].size(0)*outputs[0].size(1), -1), \n",
    "                             target_seqs.view(target_seqs.size(0)*target_seqs.size(1)))\n",
    "\n",
    "            loss_tracker.append(loss.item()*batch_size)\n",
    "            \n",
    "        except RuntimeError as exception:\n",
    "            \n",
    "            if \"out of memory\" in str(exception):\n",
    "                oom_time += 1\n",
    "                log_msg = \"WARNING: ran out of memory,times: {}\".format(oom_time)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                torch.cuda.empty_cache()\n",
    "                if hasattr(torch.cuda, 'empty_cache'):\n",
    "                    torch.cuda.empty_cache()\n",
    "            else:\n",
    "                log_msg = str(exception)\n",
    "                print(log_msg)   \n",
    "                log2file(str(experiment_trainlog), log_msg)\n",
    "                raise exception\n",
    "                \n",
    "#         torch.cuda.empty_cache()\n",
    "        pbar_dev.update(1)\n",
    "\n",
    "    total_time = time.time() - start\n",
    "\n",
    "    mean_loss = np.sum(np.array(loss_tracker)) / dev_dataset.__len__()\n",
    "    log_msg = \"{}   | Epoch {:d}/{:d} | Mean Loss {:5.5f}  | Total time cost {:d} s\"  \\\n",
    "        .format('dev'.upper(), epoch, opts.num_epochs, mean_loss, int(total_time))\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    \n",
    "    val_loss = mean_loss\n",
    "    \n",
    "    early_stopping(val_loss, model, epoch)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "    torch.save(model.state_dict(), experiment_dir / 'epoch_{}.mdl'.format(epoch))\n",
    "\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

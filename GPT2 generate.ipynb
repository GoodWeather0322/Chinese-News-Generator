{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from tqdm import tqdm_notebook, tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = \"/mnt/disk3/m10615110/gpt2_chinese/exp/\"\n",
    "gpt2_pretrain_path = \"/home/m10615110/gpt2/\"\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model.resize_token_embeddings(bert_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_sent = '10 + 20 = '\n",
    "# gpt_token = gpt_tokenizer.tokenize(start_sent)\n",
    "# gpt_idxs = gpt_tokenizer.convert_tokens_to_ids(gpt_token)\n",
    "# gpt_seqs = torch.LongTensor(gpt_idxs)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output_sequences = model.generate(gpt_seqs.unsqueeze(0), \n",
    "#                                       max_length=bert_seqs.size(0)+20, \n",
    "#                                       top_k=beam_size, \n",
    "#                                       top_p=top_p, \n",
    "#                                       repetition_penalty=repetition_penalty,\n",
    "#                                       temperature=temperature,\n",
    "#                                       do_sample = False,\n",
    "#                                       num_return_sequences=1, \n",
    "#                                       no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "#                                       pad_token_id=gpt_tokenizer.pad_token_id,\n",
    "#                                       bos_token_id=bert_tokenizer.bos_token_id,\n",
    "#                                       eos_token_id=bert_tokenizer.eos_token_id\n",
    "#                                      )\n",
    "    \n",
    "# print(' '.join(gpt_tokenizer.convert_ids_to_tokens(output_sequences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(exp_dir) / 'gpt2_medium_noptt_len512_batch_4_2020-08-11 02:45:14' / 'epoch_8.mdl'\n",
    "\n",
    "from collections import OrderedDict\n",
    "state_dict = torch.load(model_path, map_location='cpu')\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total parms : ', sum(p.numel() for p in model.parameters()))\n",
    "print('trainable parms : ', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "beam_size = 50\n",
    "repetition_penalty = 8.0\n",
    "temperature = 5.0\n",
    "top_p = 0.7\n",
    "no_repeat_ngram_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sent = '總統發表宣示致詞，'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = re.split('。|，| ', start_sent)\n",
    "while '' in sents:\n",
    "    sents.remove('')\n",
    "\n",
    "bert_sent = '[CLS]'\n",
    "for sent in sents:\n",
    "    bert_sent += sent\n",
    "    bert_sent += '[SEP]'\n",
    "\n",
    "# bert_sent = start_sent\n",
    "print(bert_sent)\n",
    "\n",
    "bert_tokens = bert_tokenizer.tokenize(bert_sent)\n",
    "# print(bert_tokens)\n",
    "bert_idxs = bert_tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "# print(bert_idxs)\n",
    "bert_seqs = torch.LongTensor(bert_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_sequences = model.generate(bert_seqs.unsqueeze(0), \n",
    "                                      max_length=bert_seqs.size(0)+max_length, \n",
    "                                      top_k=beam_size, \n",
    "                                      top_p=top_p, \n",
    "                                      repetition_penalty=repetition_penalty,\n",
    "                                      temperature=temperature,\n",
    "                                      do_sample = False,\n",
    "                                      num_return_sequences=1, \n",
    "                                      no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                      pad_token_id=bert_tokenizer.pad_token_id,\n",
    "                                      bos_token_id=bert_tokenizer.cls_token_id,\n",
    "#                                       eos_token_id=bert_tokenizer.sep_token_id\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(' '.join(bert_tokenizer.convert_ids_to_tokens(output_sequences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c16acbdfdef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'。|，| '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbert_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[CLS]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "sents = re.split('。|，| ', start_sent)\n",
    "while '' in sents:\n",
    "    sents.remove('')\n",
    "\n",
    "bert_sent = '[CLS]'\n",
    "for sent in sents:\n",
    "    bert_sent += sent\n",
    "    bert_sent += '[SEP]'\n",
    "\n",
    "# bert_sent = bert_sent[:-5]\n",
    "print(bert_sent)\n",
    "\n",
    "bert_tokens = bert_tokenizer.tokenize(bert_sent)\n",
    "bert_idxs = bert_tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_idxs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-714299e8f44f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mbeams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mbeams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbert_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtnrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_idxs' is not defined"
     ]
    }
   ],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        # torch.topk()返回最后一维最大的top_k个元素，返回值为二维(values,indices)\n",
    "        # ...表示其他维度由计算机自行推断\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value  # 对于topk之外的其他元素的logits值设为负无穷\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)  # 对logits进行递减排序\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "beams = []\n",
    "beams.append([bert_idxs, 0])\n",
    "\n",
    "for _ in tnrange(max_length):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for beam in beams:\n",
    "        \n",
    "        bert_idxs = beam[0]\n",
    "        \n",
    "        bert_seqs = torch.LongTensor(bert_idxs)\n",
    "        if USE_CUDA:\n",
    "            bert_seqs.cuda()\n",
    "        \n",
    "        outputs = model(bert_seqs)\n",
    "        next_token_logits = outputs[0][-1, :]\n",
    "        next_token_logits[bert_tokenizer.unk_token_id] = -np.inf\n",
    "        \n",
    "        # penalty for already appeared words\n",
    "        for idx in set(bert_idxs):\n",
    "            if next_token_logits[idx] > 0:\n",
    "                next_token_logits[idx] /= repetition_penalty\n",
    "            else:\n",
    "                next_token_logits[idx] *= repetition_penalty\n",
    "            \n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        \n",
    "        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=beam_size, top_p=top_p)\n",
    "\n",
    "        next_token_prob = F.log_softmax(next_token_logits, dim=0)\n",
    "        probs, idxs = torch.topk(next_token_prob, k=beam_size)\n",
    "        \n",
    "        for prob, idx in zip(probs, idxs):\n",
    "            \n",
    "            generate_idxs = bert_idxs + [idx.item()]\n",
    "            accumulate_prob = beam[1] + prob.item()\n",
    "            \n",
    "            results.append([generate_idxs, accumulate_prob])\n",
    "            \n",
    "    # beam search        \n",
    "    results.sort(key=lambda x:x[1])\n",
    "    results = results[::-1]\n",
    "    beams = results[:beam_size]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(bert_tokenizer.convert_ids_to_tokens(beams[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
